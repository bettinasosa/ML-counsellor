import { NextRequest, NextResponse } from 'next/server';import OpenAI from 'openai';import { SYSTEM_PROMPT } from "@/components/memory/system";import {marked} from "marked";const openai = new OpenAI({    apiKey: process.env.OPENAI_API_KEY,});const MAX_TOKENS = 2000;const MAX_RECENT_MESSAGES = 10;const MAX_LONG_TERM_ITEMS = 20;const INITIAL_MEMORY_MESSAGES = 3;export type MemoryItem = {    type: 'fact' | 'insight' | 'summary';    content: string;    timestamp: number;};export type Memory = {    systemPrompt: string;    recentMessages: { role: string; content: string }[];    longTermMemory: MemoryItem[];    isInitialized: boolean;};function initializeMemory(): Memory {    return {        systemPrompt: SYSTEM_PROMPT,        recentMessages: [],        longTermMemory: [],        isInitialized: false    };}export async function POST(req: NextRequest) {    let memory: Memory = initializeMemory();    try {        const { messages } = await req.json();        const newMessage = messages[messages.length - 1];        memory = await updateMemory(memory, newMessage);        const context = generateContext(memory);        // Check if we need to summarize        if (memory.isInitialized && context.split(' ').length > MAX_TOKENS) {            memory = await summarizeAndUpdateMemory(memory);        }        // Generate new context with updated memory        const newContext = generateContext(memory);        // Generate response        const completion = await openai.chat.completions.create({            model: "gpt-4",            messages: [                { role: 'system', content: newContext },                newMessage            ],            max_tokens: 500,            temperature: 0.7,        });        const response = completion.choices[0].message;        // Format the response content        const formattedContent = formatMessage(response.content || '');        // Get the latest summary        const latestSummary = memory.longTermMemory.find(item => item.type === 'summary')?.content || '';        console.log('Latest summary:', latestSummary); // Add this log        return NextResponse.json({            result: {                ...response,                content: formattedContent            },            contextInfo: {                inContextMessages: memory.recentMessages.length,                outOfContextMessages: memory.longTermMemory.length,                summary: latestSummary,            }        });    } catch (error) {        console.error('Error:', error);        return NextResponse.json({ error: 'An error occurred during your request.' }, { status: 500 });    }}async function updateMemory(memory: Memory, newMessage: { role: string; content: string }): Promise<Memory> {    const updatedRecentMessages = [newMessage, ...memory.recentMessages].slice(0, MAX_RECENT_MESSAGES);    let updatedLongTermMemory = [...memory.longTermMemory];    let isInitialized = memory.isInitialized;    if (!isInitialized && updatedRecentMessages.length >= INITIAL_MEMORY_MESSAGES) {        const initialSummary = await generateSummary(updatedRecentMessages);        updatedLongTermMemory.unshift({            type: 'summary',            content: initialSummary,            timestamp: Date.now()        });        isInitialized = true;    }    if (isInitialized && newMessage.role === 'user' && (newMessage.content.includes("I feel") || newMessage.content.includes("I think"))) {        updatedLongTermMemory.unshift({            type: 'insight',            content: newMessage.content,            timestamp: Date.now()        });    }    return {        ...memory,        recentMessages: updatedRecentMessages,        longTermMemory: updatedLongTermMemory.slice(0, MAX_LONG_TERM_ITEMS),        isInitialized    };}async function summarizeAndUpdateMemory(memory: Memory): Promise<Memory> {    const context = generateContext(memory);    const summaryPrompt = `Summarize the following conversation context concisely:\n${context}`;    const summaryCompletion = await openai.chat.completions.create({        model: "gpt-4o-mini",        messages: [{ role: 'user', content: summaryPrompt }],        max_tokens: 100,    });    const summary = summaryCompletion.choices[0].message.content ?? '';    // Add new summary to long-term memory    const updatedLongTermMemory = [        { type: 'summary' as const, content: summary, timestamp: Date.now() },        ...memory.longTermMemory    ].slice(0, MAX_LONG_TERM_ITEMS);    // Clear recent messages after summarisation    return {        ...memory,        recentMessages: [],        longTermMemory: updatedLongTermMemory,    };}async function generateSummary(messages: { role: string; content: string }[]): Promise<string> {    const context = messages.map(msg => `${msg.role}: ${msg.content}`).join('\n');    const summaryPrompt = `Summarize the following conversation context concisely:\n${context}`;    const summaryCompletion = await openai.chat.completions.create({        model: "gpt-4o-mini",        messages: [{ role: 'user', content: summaryPrompt }],        max_tokens: 100,    });    return summaryCompletion.choices[0].message.content ?? '';}function generateContext(memory: Memory): string {    const recentContext = memory.recentMessages.map(msg => `${msg.role}: ${msg.content}`).join('\n');    const longTermContext = memory.longTermMemory.map(item => `${item.type.toUpperCase()}: ${item.content}`).join('\n');    return `${memory.systemPrompt}\n\nRECENT CONTEXT:\n${recentContext}\n\nLONG-TERM MEMORY:\n${longTermContext}`;}function formatMessage(content: string): string {    // Use marked to convert markdown to HTML    let html = marked(content);    return <string>html;}