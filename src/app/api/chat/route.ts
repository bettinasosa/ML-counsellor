import { NextRequest, NextResponse } from 'next/server';import OpenAI from 'openai';import { SYSTEM_PROMPT } from "@/components/memory/system";import { marked } from "marked";const openai = new OpenAI({    apiKey: process.env.OPENAI_API_KEY,});const MAX_MEMORY_TOKENS = 1000;const SUMMARY_INTERVAL = 2;type Message = {    role: 'user' | 'assistant' | 'system';    content: string;};type Memory = {    messages: Message[];    summary: string;    messageCount: number;};let globalMemory: Memory = {    messages: [{ role: 'system', content: SYSTEM_PROMPT }],    summary: '',    messageCount: 0};export async function POST(req: NextRequest) {    let memory = globalMemory;    try {        const { messages } = await req.json();        const newMessage = messages[messages.length - 1];        memory = await updateMemory(memory, newMessage);        // This is mainly to trigger the summary manually for now;        // it will be changed once found a balance on summary frequency.        if (memory.messageCount % SUMMARY_INTERVAL === 0) {            memory = await summarizeAndUpdateMemory(memory);        }        const completion = await openai.chat.completions.create({            model: "gpt-4o-mini",            messages: [                { role: 'system', content: `${SYSTEM_PROMPT}\n\nPrevious conversation summary: ${memory.summary}` },                ...memory.messages.slice(-5)            ],            max_tokens: 500,            temperature: 0.7,        });        const response = completion.choices[0].message;        memory.messages.push({ role: 'assistant', content: response.content || '' });        globalMemory = memory;        console.log('Memory:', memory);        return NextResponse.json({            result: {                ...response,                content: formatMessage(response.content || '')            },            contextInfo: {                messagesInMemory: memory.messages.length,                summary: memory.summary,            }        });    } catch (error) {        console.error('Error:', error);        return NextResponse.json({ error: 'An error occurred during your request.' }, { status: 500 });    }}async function updateMemory(memory: Memory, newMessage: Message): Promise<Memory> {    memory.messages.push(newMessage);    memory.messageCount++;    while (estimateTokens(JSON.stringify(memory)) > MAX_MEMORY_TOKENS) {        if (memory.messages.length > 1) {            memory.messages.shift();        } else {            break;        }    }    return memory;}async function summarizeAndUpdateMemory(memory: Memory): Promise<Memory> {    const context = memory.messages.map(msg => `${msg.role}: ${msg.content}`).join('\n');    const summaryPrompt = `Summarize the following conversation context concisely, including both user messages and AI responses, do not add assumptions or ideas, export in bullet points if necessary:\n${context}`;    const summaryCompletion = await openai.chat.completions.create({        model: "gpt-4o-mini",        messages: [{ role: 'user', content: summaryPrompt }],        max_tokens: 150,    });    memory.summary = summaryCompletion.choices[0].message.content ?? '';    return memory;}function estimateTokens(text: string): number {    // This is a very rough estimate. Should use a tokenizer.    return text.split(/\s+/).length;}function formatMessage(content: string): string {    return <string>marked(content);}